{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MobileNet v3\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=512, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=200, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print('==> Building model..')\n",
    "\n",
    "model = torchvision.models.mobilenet_v3_large()\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "if device == 'cuda':\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(trainloader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            #loss = nn.NLLLoss(output,target)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0:\n",
    "                done = batch_idx * len(data)\n",
    "                percentage = 100. * batch_idx / len(trainloader)\n",
    "                print(f'Train Epoch: {epoch} [{done:5}/{len(trainloader.dataset)} ({percentage:3.0f}%)]  Loss: {loss.item():.6f}')\n",
    "\n",
    "        test(trainloader)\n",
    "        test(testloader)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            #test_loss += nn.NLLLoss(output, target).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(loader.dataset)\n",
    "        accuracy = 100. * correct / len(loader.dataset)\n",
    "        print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(loader.dataset)} ({accuracy:.2f}%)')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module Hardswish is treated as a zero-op.\n",
      "Warning: module Conv2dNormActivation is treated as a zero-op.\n",
      "Warning: module InvertedResidual is treated as a zero-op.\n",
      "Warning: module Hardsigmoid is treated as a zero-op.\n",
      "Warning: module SqueezeExcitation is treated as a zero-op.\n",
      "Warning: module Dropout is treated as a zero-op.\n",
      "Warning: module MobileNetV3 is treated as a zero-op.\n",
      "Warning: module DataParallel is treated as a zero-op.\n",
      "DataParallel(\n",
      "  5.48 M, 100.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "  (module): MobileNetV3(\n",
      "    5.48 M, 100.000% Params, 0.0 Mac, 0.000% MACs, \n",
      "    (features): Sequential(\n",
      "      2.97 M, 54.203% Params, 0.0 Mac, 0.000% MACs, \n",
      "      (0): Conv2dNormActivation(\n",
      "        464, 0.008% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (0): Conv2d(432, 0.008% Params, 0.0 Mac, 0.000% MACs, 3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(32, 0.001% Params, 0.0 Mac, 0.000% MACs, 16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "      )\n",
      "      (1): InvertedResidual(\n",
      "        464, 0.008% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (block): Sequential(\n",
      "          464, 0.008% Params, 0.0 Mac, 0.000% MACs, \n",
      "          (0): Conv2dNormActivation(\n",
      "            176, 0.003% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(144, 0.003% Params, 0.0 Mac, 0.000% MACs, 16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "            (1): BatchNorm2d(32, 0.001% Params, 0.0 Mac, 0.000% MACs, 16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            288, 0.005% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(256, 0.005% Params, 0.0 Mac, 0.000% MACs, 16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(32, 0.001% Params, 0.0 Mac, 0.000% MACs, 16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): InvertedResidual(\n",
      "        3.44 k, 0.063% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (block): Sequential(\n",
      "          3.44 k, 0.063% Params, 0.0 Mac, 0.000% MACs, \n",
      "          (0): Conv2dNormActivation(\n",
      "            1.15 k, 0.021% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(1.02 k, 0.019% Params, 0.0 Mac, 0.000% MACs, 16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(128, 0.002% Params, 0.0 Mac, 0.000% MACs, 64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            704, 0.013% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(576, 0.011% Params, 0.0 Mac, 0.000% MACs, 64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "            (1): BatchNorm2d(128, 0.002% Params, 0.0 Mac, 0.000% MACs, 64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "          )\n",
      "          (2): Conv2dNormActivation(\n",
      "            1.58 k, 0.029% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(1.54 k, 0.028% Params, 0.0 Mac, 0.000% MACs, 64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(48, 0.001% Params, 0.0 Mac, 0.000% MACs, 24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): InvertedResidual(\n",
      "        4.44 k, 0.081% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (block): Sequential(\n",
      "          4.44 k, 0.081% Params, 0.0 Mac, 0.000% MACs, \n",
      "          (0): Conv2dNormActivation(\n",
      "            1.87 k, 0.034% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(1.73 k, 0.032% Params, 0.0 Mac, 0.000% MACs, 24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, 0.003% Params, 0.0 Mac, 0.000% MACs, 72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            792, 0.014% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(648, 0.012% Params, 0.0 Mac, 0.000% MACs, 72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
      "            (1): BatchNorm2d(144, 0.003% Params, 0.0 Mac, 0.000% MACs, 72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "          )\n",
      "          (2): Conv2dNormActivation(\n",
      "            1.78 k, 0.032% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(1.73 k, 0.032% Params, 0.0 Mac, 0.000% MACs, 72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(48, 0.001% Params, 0.0 Mac, 0.000% MACs, 24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): InvertedResidual(\n",
      "        10.33 k, 0.188% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (block): Sequential(\n",
      "          10.33 k, 0.188% Params, 0.0 Mac, 0.000% MACs, \n",
      "          (0): Conv2dNormActivation(\n",
      "            1.87 k, 0.034% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(1.73 k, 0.032% Params, 0.0 Mac, 0.000% MACs, 24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(144, 0.003% Params, 0.0 Mac, 0.000% MACs, 72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            1.94 k, 0.035% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(1.8 k, 0.033% Params, 0.0 Mac, 0.000% MACs, 72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
      "            (1): BatchNorm2d(144, 0.003% Params, 0.0 Mac, 0.000% MACs, 72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            3.55 k, 0.065% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 0.0 Mac, 0.000% MACs, output_size=1)\n",
      "            (fc1): Conv2d(1.75 k, 0.032% Params, 0.0 Mac, 0.000% MACs, 72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(1.8 k, 0.033% Params, 0.0 Mac, 0.000% MACs, 24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "            (scale_activation): Hardsigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            2.96 k, 0.054% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(2.88 k, 0.053% Params, 0.0 Mac, 0.000% MACs, 72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80, 0.001% Params, 0.0 Mac, 0.000% MACs, 40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): InvertedResidual(\n",
      "        20.99 k, 0.383% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (block): Sequential(\n",
      "          20.99 k, 0.383% Params, 0.0 Mac, 0.000% MACs, \n",
      "          (0): Conv2dNormActivation(\n",
      "            5.04 k, 0.092% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(4.8 k, 0.088% Params, 0.0 Mac, 0.000% MACs, 40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(240, 0.004% Params, 0.0 Mac, 0.000% MACs, 120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            3.24 k, 0.059% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(3.0 k, 0.055% Params, 0.0 Mac, 0.000% MACs, 120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "            (1): BatchNorm2d(240, 0.004% Params, 0.0 Mac, 0.000% MACs, 120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            7.83 k, 0.143% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 0.0 Mac, 0.000% MACs, output_size=1)\n",
      "            (fc1): Conv2d(3.87 k, 0.071% Params, 0.0 Mac, 0.000% MACs, 120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(3.96 k, 0.072% Params, 0.0 Mac, 0.000% MACs, 32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "            (scale_activation): Hardsigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            4.88 k, 0.089% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(4.8 k, 0.088% Params, 0.0 Mac, 0.000% MACs, 120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80, 0.001% Params, 0.0 Mac, 0.000% MACs, 40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): InvertedResidual(\n",
      "        20.99 k, 0.383% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (block): Sequential(\n",
      "          20.99 k, 0.383% Params, 0.0 Mac, 0.000% MACs, \n",
      "          (0): Conv2dNormActivation(\n",
      "            5.04 k, 0.092% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(4.8 k, 0.088% Params, 0.0 Mac, 0.000% MACs, 40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(240, 0.004% Params, 0.0 Mac, 0.000% MACs, 120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            3.24 k, 0.059% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(3.0 k, 0.055% Params, 0.0 Mac, 0.000% MACs, 120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "            (1): BatchNorm2d(240, 0.004% Params, 0.0 Mac, 0.000% MACs, 120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, inplace=True)\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            7.83 k, 0.143% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 0.0 Mac, 0.000% MACs, output_size=1)\n",
      "            (fc1): Conv2d(3.87 k, 0.071% Params, 0.0 Mac, 0.000% MACs, 120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(3.96 k, 0.072% Params, 0.0 Mac, 0.000% MACs, 32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "            (scale_activation): Hardsigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            4.88 k, 0.089% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(4.8 k, 0.088% Params, 0.0 Mac, 0.000% MACs, 120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(80, 0.001% Params, 0.0 Mac, 0.000% MACs, 40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): InvertedResidual(\n",
      "        32.08 k, 0.585% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (block): Sequential(\n",
      "          32.08 k, 0.585% Params, 0.0 Mac, 0.000% MACs, \n",
      "          (0): Conv2dNormActivation(\n",
      "            10.08 k, 0.184% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(9.6 k, 0.175% Params, 0.0 Mac, 0.000% MACs, 40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(480, 0.009% Params, 0.0 Mac, 0.000% MACs, 240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): Hardswish(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            2.64 k, 0.048% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(2.16 k, 0.039% Params, 0.0 Mac, 0.000% MACs, 240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "            (1): BatchNorm2d(480, 0.009% Params, 0.0 Mac, 0.000% MACs, 240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): Hardswish(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "          )\n",
      "          (2): Conv2dNormActivation(\n",
      "            19.36 k, 0.353% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(19.2 k, 0.350% Params, 0.0 Mac, 0.000% MACs, 240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(160, 0.003% Params, 0.0 Mac, 0.000% MACs, 80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): InvertedResidual(\n",
      "        34.76 k, 0.634% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (block): Sequential(\n",
      "          34.76 k, 0.634% Params, 0.0 Mac, 0.000% MACs, \n",
      "          (0): Conv2dNormActivation(\n",
      "            16.4 k, 0.299% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(16.0 k, 0.292% Params, 0.0 Mac, 0.000% MACs, 80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(400, 0.007% Params, 0.0 Mac, 0.000% MACs, 200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): Hardswish(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            2.2 k, 0.040% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(1.8 k, 0.033% Params, 0.0 Mac, 0.000% MACs, 200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
      "            (1): BatchNorm2d(400, 0.007% Params, 0.0 Mac, 0.000% MACs, 200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): Hardswish(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "          )\n",
      "          (2): Conv2dNormActivation(\n",
      "            16.16 k, 0.295% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(16.0 k, 0.292% Params, 0.0 Mac, 0.000% MACs, 200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(160, 0.003% Params, 0.0 Mac, 0.000% MACs, 80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): InvertedResidual(\n",
      "        31.99 k, 0.583% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (block): Sequential(\n",
      "          31.99 k, 0.583% Params, 0.0 Mac, 0.000% MACs, \n",
      "          (0): Conv2dNormActivation(\n",
      "            15.09 k, 0.275% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(14.72 k, 0.268% Params, 0.0 Mac, 0.000% MACs, 80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(368, 0.007% Params, 0.0 Mac, 0.000% MACs, 184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): Hardswish(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            2.02 k, 0.037% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(1.66 k, 0.030% Params, 0.0 Mac, 0.000% MACs, 184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "            (1): BatchNorm2d(368, 0.007% Params, 0.0 Mac, 0.000% MACs, 184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): Hardswish(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "          )\n",
      "          (2): Conv2dNormActivation(\n",
      "            14.88 k, 0.271% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(14.72 k, 0.268% Params, 0.0 Mac, 0.000% MACs, 184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(160, 0.003% Params, 0.0 Mac, 0.000% MACs, 80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): InvertedResidual(\n",
      "        31.99 k, 0.583% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (block): Sequential(\n",
      "          31.99 k, 0.583% Params, 0.0 Mac, 0.000% MACs, \n",
      "          (0): Conv2dNormActivation(\n",
      "            15.09 k, 0.275% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(14.72 k, 0.268% Params, 0.0 Mac, 0.000% MACs, 80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(368, 0.007% Params, 0.0 Mac, 0.000% MACs, 184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): Hardswish(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            2.02 k, 0.037% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(1.66 k, 0.030% Params, 0.0 Mac, 0.000% MACs, 184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "            (1): BatchNorm2d(368, 0.007% Params, 0.0 Mac, 0.000% MACs, 184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): Hardswish(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "          )\n",
      "          (2): Conv2dNormActivation(\n",
      "            14.88 k, 0.271% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(14.72 k, 0.268% Params, 0.0 Mac, 0.000% MACs, 184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(160, 0.003% Params, 0.0 Mac, 0.000% MACs, 80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): InvertedResidual(\n",
      "        214.42 k, 3.911% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (block): Sequential(\n",
      "          214.42 k, 3.911% Params, 0.0 Mac, 0.000% MACs, \n",
      "          (0): Conv2dNormActivation(\n",
      "            39.36 k, 0.718% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(38.4 k, 0.700% Params, 0.0 Mac, 0.000% MACs, 80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(960, 0.018% Params, 0.0 Mac, 0.000% MACs, 480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): Hardswish(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            5.28 k, 0.096% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(4.32 k, 0.079% Params, 0.0 Mac, 0.000% MACs, 480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "            (1): BatchNorm2d(960, 0.018% Params, 0.0 Mac, 0.000% MACs, 480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): Hardswish(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            115.8 k, 2.112% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 0.0 Mac, 0.000% MACs, output_size=1)\n",
      "            (fc1): Conv2d(57.72 k, 1.053% Params, 0.0 Mac, 0.000% MACs, 480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(58.08 k, 1.059% Params, 0.0 Mac, 0.000% MACs, 120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "            (scale_activation): Hardsigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            53.98 k, 0.985% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(53.76 k, 0.980% Params, 0.0 Mac, 0.000% MACs, 480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(224, 0.004% Params, 0.0 Mac, 0.000% MACs, 112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (12): InvertedResidual(\n",
      "        386.12 k, 7.042% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (block): Sequential(\n",
      "          386.12 k, 7.042% Params, 0.0 Mac, 0.000% MACs, \n",
      "          (0): Conv2dNormActivation(\n",
      "            76.61 k, 1.397% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(75.26 k, 1.373% Params, 0.0 Mac, 0.000% MACs, 112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1.34 k, 0.025% Params, 0.0 Mac, 0.000% MACs, 672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): Hardswish(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            7.39 k, 0.135% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(6.05 k, 0.110% Params, 0.0 Mac, 0.000% MACs, 672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(1.34 k, 0.025% Params, 0.0 Mac, 0.000% MACs, 672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): Hardswish(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            226.63 k, 4.133% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 0.0 Mac, 0.000% MACs, output_size=1)\n",
      "            (fc1): Conv2d(113.06 k, 2.062% Params, 0.0 Mac, 0.000% MACs, 672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(113.57 k, 2.071% Params, 0.0 Mac, 0.000% MACs, 168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "            (scale_activation): Hardsigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            75.49 k, 1.377% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(75.26 k, 1.373% Params, 0.0 Mac, 0.000% MACs, 672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(224, 0.004% Params, 0.0 Mac, 0.000% MACs, 112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): InvertedResidual(\n",
      "        429.22 k, 7.828% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (block): Sequential(\n",
      "          429.22 k, 7.828% Params, 0.0 Mac, 0.000% MACs, \n",
      "          (0): Conv2dNormActivation(\n",
      "            76.61 k, 1.397% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(75.26 k, 1.373% Params, 0.0 Mac, 0.000% MACs, 112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1.34 k, 0.025% Params, 0.0 Mac, 0.000% MACs, 672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): Hardswish(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            18.14 k, 0.331% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(16.8 k, 0.306% Params, 0.0 Mac, 0.000% MACs, 672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "            (1): BatchNorm2d(1.34 k, 0.025% Params, 0.0 Mac, 0.000% MACs, 672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): Hardswish(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            226.63 k, 4.133% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 0.0 Mac, 0.000% MACs, output_size=1)\n",
      "            (fc1): Conv2d(113.06 k, 2.062% Params, 0.0 Mac, 0.000% MACs, 672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(113.57 k, 2.071% Params, 0.0 Mac, 0.000% MACs, 168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "            (scale_activation): Hardsigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            107.84 k, 1.967% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(107.52 k, 1.961% Params, 0.0 Mac, 0.000% MACs, 672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(320, 0.006% Params, 0.0 Mac, 0.000% MACs, 160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): InvertedResidual(\n",
      "        797.36 k, 14.542% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (block): Sequential(\n",
      "          797.36 k, 14.542% Params, 0.0 Mac, 0.000% MACs, \n",
      "          (0): Conv2dNormActivation(\n",
      "            155.52 k, 2.836% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(153.6 k, 2.801% Params, 0.0 Mac, 0.000% MACs, 160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1.92 k, 0.035% Params, 0.0 Mac, 0.000% MACs, 960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): Hardswish(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            25.92 k, 0.473% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(24.0 k, 0.438% Params, 0.0 Mac, 0.000% MACs, 960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "            (1): BatchNorm2d(1.92 k, 0.035% Params, 0.0 Mac, 0.000% MACs, 960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): Hardswish(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            462.0 k, 8.426% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 0.0 Mac, 0.000% MACs, output_size=1)\n",
      "            (fc1): Conv2d(230.64 k, 4.206% Params, 0.0 Mac, 0.000% MACs, 960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(231.36 k, 4.220% Params, 0.0 Mac, 0.000% MACs, 240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "            (scale_activation): Hardsigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            153.92 k, 2.807% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(153.6 k, 2.801% Params, 0.0 Mac, 0.000% MACs, 960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(320, 0.006% Params, 0.0 Mac, 0.000% MACs, 160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (15): InvertedResidual(\n",
      "        797.36 k, 14.542% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (block): Sequential(\n",
      "          797.36 k, 14.542% Params, 0.0 Mac, 0.000% MACs, \n",
      "          (0): Conv2dNormActivation(\n",
      "            155.52 k, 2.836% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(153.6 k, 2.801% Params, 0.0 Mac, 0.000% MACs, 160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(1.92 k, 0.035% Params, 0.0 Mac, 0.000% MACs, 960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): Hardswish(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "          )\n",
      "          (1): Conv2dNormActivation(\n",
      "            25.92 k, 0.473% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(24.0 k, 0.438% Params, 0.0 Mac, 0.000% MACs, 960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "            (1): BatchNorm2d(1.92 k, 0.035% Params, 0.0 Mac, 0.000% MACs, 960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            (2): Hardswish(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "          )\n",
      "          (2): SqueezeExcitation(\n",
      "            462.0 k, 8.426% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 0.0 Mac, 0.000% MACs, output_size=1)\n",
      "            (fc1): Conv2d(230.64 k, 4.206% Params, 0.0 Mac, 0.000% MACs, 960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (fc2): Conv2d(231.36 k, 4.220% Params, 0.0 Mac, 0.000% MACs, 240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (activation): ReLU(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "            (scale_activation): Hardsigmoid(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "          )\n",
      "          (3): Conv2dNormActivation(\n",
      "            153.92 k, 2.807% Params, 0.0 Mac, 0.000% MACs, \n",
      "            (0): Conv2d(153.6 k, 2.801% Params, 0.0 Mac, 0.000% MACs, 960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(320, 0.006% Params, 0.0 Mac, 0.000% MACs, 160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): Conv2dNormActivation(\n",
      "        155.52 k, 2.836% Params, 0.0 Mac, 0.000% MACs, \n",
      "        (0): Conv2d(153.6 k, 2.801% Params, 0.0 Mac, 0.000% MACs, 160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1.92 k, 0.035% Params, 0.0 Mac, 0.000% MACs, 960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 0.0 Mac, 0.000% MACs, output_size=1)\n",
      "    (classifier): Sequential(\n",
      "      2.51 M, 45.797% Params, 0.0 Mac, 0.000% MACs, \n",
      "      (0): Linear(1.23 M, 22.434% Params, 0.0 Mac, 0.000% MACs, in_features=960, out_features=1280, bias=True)\n",
      "      (1): Hardswish(0, 0.000% Params, 0.0 Mac, 0.000% MACs, )\n",
      "      (2): Dropout(0, 0.000% Params, 0.0 Mac, 0.000% MACs, p=0.2, inplace=True)\n",
      "      (3): Linear(1.28 M, 23.363% Params, 0.0 Mac, 0.000% MACs, in_features=1280, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Computational complexity:       61.41 KMac\n",
      "Number of parameters:           5.48 M  \n"
     ]
    }
   ],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "with torch.cuda.device(0):\n",
    "  macs, params = get_model_complexity_info(model, (3,32,32), as_strings=True, backend='pytorch',\n",
    "                                           print_per_layer_stat=True, verbose=True)\n",
    "  print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "  print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [    0/50000 (  0%)]  Loss: 6.909417\n",
      "Test set: Average loss: 0.0080, Accuracy: 5000/50000 (10.00%)\n",
      "Test set: Average loss: 0.0203, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 1 [    0/50000 (  0%)]  Loss: 4.068769\n",
      "Test set: Average loss: 0.0045, Accuracy: 5000/50000 (10.00%)\n",
      "Test set: Average loss: 0.0116, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 2 [    0/50000 (  0%)]  Loss: 2.312489\n",
      "Test set: Average loss: 0.0045, Accuracy: 5000/50000 (10.00%)\n",
      "Test set: Average loss: 0.0116, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 3 [    0/50000 (  0%)]  Loss: 2.304975\n",
      "Test set: Average loss: 0.0045, Accuracy: 5000/50000 (10.00%)\n",
      "Test set: Average loss: 0.0116, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 4 [    0/50000 (  0%)]  Loss: 2.305122\n",
      "Test set: Average loss: 0.0045, Accuracy: 5000/50000 (10.00%)\n",
      "Test set: Average loss: 0.0115, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 5 [    0/50000 (  0%)]  Loss: 2.311495\n",
      "Test set: Average loss: 0.0045, Accuracy: 5000/50000 (10.00%)\n",
      "Test set: Average loss: 0.0115, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 6 [    0/50000 (  0%)]  Loss: 2.302950\n",
      "Test set: Average loss: 0.0045, Accuracy: 5000/50000 (10.00%)\n",
      "Test set: Average loss: 0.0115, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 7 [    0/50000 (  0%)]  Loss: 2.309121\n",
      "Test set: Average loss: 0.0045, Accuracy: 5000/50000 (10.00%)\n",
      "Test set: Average loss: 0.0116, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 8 [    0/50000 (  0%)]  Loss: 2.301238\n",
      "Test set: Average loss: 0.0045, Accuracy: 5000/50000 (10.00%)\n",
      "Test set: Average loss: 0.0116, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 9 [    0/50000 (  0%)]  Loss: 2.316102\n",
      "Test set: Average loss: 0.0045, Accuracy: 5000/50000 (10.00%)\n",
      "Test set: Average loss: 0.0115, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 10 [    0/50000 (  0%)]  Loss: 2.305531\n",
      "Test set: Average loss: 0.0045, Accuracy: 5000/50000 (10.00%)\n",
      "Test set: Average loss: 0.0116, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 11 [    0/50000 (  0%)]  Loss: 2.311374\n",
      "Test set: Average loss: 0.0045, Accuracy: 5000/50000 (10.00%)\n",
      "Test set: Average loss: 0.0116, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 12 [    0/50000 (  0%)]  Loss: 2.316275\n",
      "Test set: Average loss: 0.0045, Accuracy: 5000/50000 (10.00%)\n",
      "Test set: Average loss: 0.0116, Accuracy: 1000/10000 (10.00%)\n",
      "Train Epoch: 13 [    0/50000 (  0%)]  Loss: 2.315450\n",
      "Test set: Average loss: 0.0045, Accuracy: 5268/50000 (10.54%)\n",
      "Test set: Average loss: 0.0115, Accuracy: 1117/10000 (11.17%)\n",
      "Train Epoch: 14 [    0/50000 (  0%)]  Loss: 2.307132\n",
      "Test set: Average loss: 0.0041, Accuracy: 9967/50000 (19.93%)\n",
      "Test set: Average loss: 0.0105, Accuracy: 1929/10000 (19.29%)\n",
      "Train Epoch: 15 [    0/50000 (  0%)]  Loss: 2.064162\n",
      "Test set: Average loss: 0.0037, Accuracy: 11837/50000 (23.67%)\n",
      "Test set: Average loss: 0.0094, Accuracy: 2407/10000 (24.07%)\n",
      "Train Epoch: 16 [    0/50000 (  0%)]  Loss: 1.903978\n",
      "Test set: Average loss: 0.0036, Accuracy: 14055/50000 (28.11%)\n",
      "Test set: Average loss: 0.0093, Accuracy: 2767/10000 (27.67%)\n",
      "Train Epoch: 17 [    0/50000 (  0%)]  Loss: 1.833102\n",
      "Test set: Average loss: 0.0034, Accuracy: 15978/50000 (31.96%)\n",
      "Test set: Average loss: 0.0086, Accuracy: 3225/10000 (32.25%)\n",
      "Train Epoch: 18 [    0/50000 (  0%)]  Loss: 1.691211\n",
      "Test set: Average loss: 0.0032, Accuracy: 18160/50000 (36.32%)\n",
      "Test set: Average loss: 0.0081, Accuracy: 3776/10000 (37.76%)\n",
      "Train Epoch: 19 [    0/50000 (  0%)]  Loss: 1.671563\n",
      "Test set: Average loss: 0.0032, Accuracy: 18658/50000 (37.32%)\n",
      "Test set: Average loss: 0.0080, Accuracy: 3786/10000 (37.86%)\n",
      "Train Epoch: 20 [    0/50000 (  0%)]  Loss: 1.595766\n",
      "Test set: Average loss: 0.0030, Accuracy: 20778/50000 (41.56%)\n",
      "Test set: Average loss: 0.0076, Accuracy: 4227/10000 (42.27%)\n",
      "Train Epoch: 21 [    0/50000 (  0%)]  Loss: 1.506656\n",
      "Test set: Average loss: 0.0029, Accuracy: 21514/50000 (43.03%)\n",
      "Test set: Average loss: 0.0074, Accuracy: 4369/10000 (43.69%)\n",
      "Train Epoch: 22 [    0/50000 (  0%)]  Loss: 1.488160\n",
      "Test set: Average loss: 0.0028, Accuracy: 23264/50000 (46.53%)\n",
      "Test set: Average loss: 0.0072, Accuracy: 4665/10000 (46.65%)\n",
      "Train Epoch: 23 [    0/50000 (  0%)]  Loss: 1.469812\n",
      "Test set: Average loss: 0.0029, Accuracy: 23087/50000 (46.17%)\n",
      "Test set: Average loss: 0.0075, Accuracy: 4535/10000 (45.35%)\n",
      "Train Epoch: 24 [    0/50000 (  0%)]  Loss: 1.575737\n",
      "Test set: Average loss: 0.0028, Accuracy: 24080/50000 (48.16%)\n",
      "Test set: Average loss: 0.0072, Accuracy: 4927/10000 (49.27%)\n",
      "Train Epoch: 25 [    0/50000 (  0%)]  Loss: 1.439592\n",
      "Test set: Average loss: 0.0026, Accuracy: 25305/50000 (50.61%)\n",
      "Test set: Average loss: 0.0070, Accuracy: 4958/10000 (49.58%)\n",
      "Train Epoch: 26 [    0/50000 (  0%)]  Loss: 1.292188\n",
      "Test set: Average loss: 0.0026, Accuracy: 25647/50000 (51.29%)\n",
      "Test set: Average loss: 0.0066, Accuracy: 5204/10000 (52.04%)\n",
      "Train Epoch: 27 [    0/50000 (  0%)]  Loss: 1.289094\n",
      "Test set: Average loss: 0.0025, Accuracy: 26515/50000 (53.03%)\n",
      "Test set: Average loss: 0.0063, Accuracy: 5424/10000 (54.24%)\n",
      "Train Epoch: 28 [    0/50000 (  0%)]  Loss: 1.175613\n",
      "Test set: Average loss: 0.0024, Accuracy: 28162/50000 (56.32%)\n",
      "Test set: Average loss: 0.0060, Accuracy: 5709/10000 (57.09%)\n",
      "Train Epoch: 29 [    0/50000 (  0%)]  Loss: 1.199061\n",
      "Test set: Average loss: 0.0024, Accuracy: 28003/50000 (56.01%)\n",
      "Test set: Average loss: 0.0059, Accuracy: 5772/10000 (57.72%)\n",
      "Train Epoch: 30 [    0/50000 (  0%)]  Loss: 1.239319\n",
      "Test set: Average loss: 0.0023, Accuracy: 29177/50000 (58.35%)\n",
      "Test set: Average loss: 0.0057, Accuracy: 5890/10000 (58.90%)\n",
      "Train Epoch: 31 [    0/50000 (  0%)]  Loss: 1.095111\n",
      "Test set: Average loss: 0.0022, Accuracy: 29954/50000 (59.91%)\n",
      "Test set: Average loss: 0.0055, Accuracy: 6047/10000 (60.47%)\n",
      "Train Epoch: 32 [    0/50000 (  0%)]  Loss: 1.100643\n",
      "Test set: Average loss: 0.0022, Accuracy: 29359/50000 (58.72%)\n",
      "Test set: Average loss: 0.0056, Accuracy: 5970/10000 (59.70%)\n",
      "Train Epoch: 33 [    0/50000 (  0%)]  Loss: 1.186256\n",
      "Test set: Average loss: 0.0021, Accuracy: 30691/50000 (61.38%)\n",
      "Test set: Average loss: 0.0054, Accuracy: 6196/10000 (61.96%)\n",
      "Train Epoch: 34 [    0/50000 (  0%)]  Loss: 1.051494\n",
      "Test set: Average loss: 0.0022, Accuracy: 30040/50000 (60.08%)\n",
      "Test set: Average loss: 0.0056, Accuracy: 6051/10000 (60.51%)\n",
      "Train Epoch: 35 [    0/50000 (  0%)]  Loss: 1.128273\n",
      "Test set: Average loss: 0.0021, Accuracy: 30735/50000 (61.47%)\n",
      "Test set: Average loss: 0.0054, Accuracy: 6145/10000 (61.45%)\n",
      "Train Epoch: 36 [    0/50000 (  0%)]  Loss: 1.130299\n",
      "Test set: Average loss: 0.0020, Accuracy: 31735/50000 (63.47%)\n",
      "Test set: Average loss: 0.0051, Accuracy: 6382/10000 (63.82%)\n",
      "Train Epoch: 37 [    0/50000 (  0%)]  Loss: 1.066158\n",
      "Test set: Average loss: 0.0019, Accuracy: 32295/50000 (64.59%)\n",
      "Test set: Average loss: 0.0050, Accuracy: 6367/10000 (63.67%)\n",
      "Train Epoch: 38 [    0/50000 (  0%)]  Loss: 1.051162\n",
      "Test set: Average loss: 0.0019, Accuracy: 32439/50000 (64.88%)\n",
      "Test set: Average loss: 0.0049, Accuracy: 6480/10000 (64.80%)\n",
      "Train Epoch: 39 [    0/50000 (  0%)]  Loss: 0.914600\n",
      "Test set: Average loss: 0.0019, Accuracy: 32340/50000 (64.68%)\n",
      "Test set: Average loss: 0.0050, Accuracy: 6471/10000 (64.71%)\n",
      "Train Epoch: 40 [    0/50000 (  0%)]  Loss: 1.089431\n",
      "Test set: Average loss: 0.0019, Accuracy: 32796/50000 (65.59%)\n",
      "Test set: Average loss: 0.0048, Accuracy: 6578/10000 (65.78%)\n",
      "Train Epoch: 41 [    0/50000 (  0%)]  Loss: 0.985644\n",
      "Test set: Average loss: 0.0018, Accuracy: 33632/50000 (67.26%)\n",
      "Test set: Average loss: 0.0048, Accuracy: 6634/10000 (66.34%)\n",
      "Train Epoch: 42 [    0/50000 (  0%)]  Loss: 1.008292\n",
      "Test set: Average loss: 0.0019, Accuracy: 33239/50000 (66.48%)\n",
      "Test set: Average loss: 0.0048, Accuracy: 6627/10000 (66.27%)\n",
      "Train Epoch: 43 [    0/50000 (  0%)]  Loss: 1.047270\n",
      "Test set: Average loss: 0.0018, Accuracy: 33588/50000 (67.18%)\n",
      "Test set: Average loss: 0.0046, Accuracy: 6709/10000 (67.09%)\n",
      "Train Epoch: 44 [    0/50000 (  0%)]  Loss: 0.986967\n",
      "Test set: Average loss: 0.0017, Accuracy: 34605/50000 (69.21%)\n",
      "Test set: Average loss: 0.0043, Accuracy: 6888/10000 (68.88%)\n",
      "Train Epoch: 45 [    0/50000 (  0%)]  Loss: 0.864683\n",
      "Test set: Average loss: 0.0017, Accuracy: 34528/50000 (69.06%)\n",
      "Test set: Average loss: 0.0044, Accuracy: 6870/10000 (68.70%)\n",
      "Train Epoch: 46 [    0/50000 (  0%)]  Loss: 0.813452\n",
      "Test set: Average loss: 0.0018, Accuracy: 33686/50000 (67.37%)\n",
      "Test set: Average loss: 0.0045, Accuracy: 6790/10000 (67.90%)\n",
      "Train Epoch: 47 [    0/50000 (  0%)]  Loss: 0.917693\n",
      "Test set: Average loss: 0.0018, Accuracy: 34252/50000 (68.50%)\n",
      "Test set: Average loss: 0.0046, Accuracy: 6815/10000 (68.15%)\n",
      "Train Epoch: 48 [    0/50000 (  0%)]  Loss: 0.889292\n",
      "Test set: Average loss: 0.0019, Accuracy: 33438/50000 (66.88%)\n",
      "Test set: Average loss: 0.0049, Accuracy: 6738/10000 (67.38%)\n",
      "Train Epoch: 49 [    0/50000 (  0%)]  Loss: 0.931650\n",
      "Test set: Average loss: 0.0017, Accuracy: 34965/50000 (69.93%)\n",
      "Test set: Average loss: 0.0044, Accuracy: 6943/10000 (69.43%)\n",
      "Train Epoch: 50 [    0/50000 (  0%)]  Loss: 0.903888\n",
      "Test set: Average loss: 0.0016, Accuracy: 35336/50000 (70.67%)\n",
      "Test set: Average loss: 0.0042, Accuracy: 7063/10000 (70.63%)\n",
      "Train Epoch: 51 [    0/50000 (  0%)]  Loss: 0.828154\n",
      "Test set: Average loss: 0.0016, Accuracy: 35617/50000 (71.23%)\n",
      "Test set: Average loss: 0.0041, Accuracy: 7141/10000 (71.41%)\n",
      "Train Epoch: 52 [    0/50000 (  0%)]  Loss: 0.838317\n",
      "Test set: Average loss: 0.0017, Accuracy: 35156/50000 (70.31%)\n",
      "Test set: Average loss: 0.0042, Accuracy: 7002/10000 (70.02%)\n",
      "Train Epoch: 53 [    0/50000 (  0%)]  Loss: 0.800738\n",
      "Test set: Average loss: 0.0018, Accuracy: 33673/50000 (67.35%)\n",
      "Test set: Average loss: 0.0048, Accuracy: 6753/10000 (67.53%)\n",
      "Train Epoch: 54 [    0/50000 (  0%)]  Loss: 0.893699\n",
      "Test set: Average loss: 0.0016, Accuracy: 35823/50000 (71.65%)\n",
      "Test set: Average loss: 0.0041, Accuracy: 7091/10000 (70.91%)\n",
      "Train Epoch: 55 [    0/50000 (  0%)]  Loss: 0.777931\n",
      "Test set: Average loss: 0.0016, Accuracy: 35780/50000 (71.56%)\n",
      "Test set: Average loss: 0.0041, Accuracy: 7160/10000 (71.60%)\n",
      "Train Epoch: 56 [    0/50000 (  0%)]  Loss: 0.700584\n",
      "Test set: Average loss: 0.0016, Accuracy: 35524/50000 (71.05%)\n",
      "Test set: Average loss: 0.0042, Accuracy: 7077/10000 (70.77%)\n",
      "Train Epoch: 57 [    0/50000 (  0%)]  Loss: 0.756727\n",
      "Test set: Average loss: 0.0015, Accuracy: 36734/50000 (73.47%)\n",
      "Test set: Average loss: 0.0039, Accuracy: 7327/10000 (73.27%)\n",
      "Train Epoch: 58 [    0/50000 (  0%)]  Loss: 0.773438\n",
      "Test set: Average loss: 0.0015, Accuracy: 36347/50000 (72.69%)\n",
      "Test set: Average loss: 0.0040, Accuracy: 7173/10000 (71.73%)\n",
      "Train Epoch: 59 [    0/50000 (  0%)]  Loss: 0.746929\n",
      "Test set: Average loss: 0.0016, Accuracy: 36141/50000 (72.28%)\n",
      "Test set: Average loss: 0.0041, Accuracy: 7140/10000 (71.40%)\n",
      "Train Epoch: 60 [    0/50000 (  0%)]  Loss: 0.796071\n",
      "Test set: Average loss: 0.0014, Accuracy: 37111/50000 (74.22%)\n",
      "Test set: Average loss: 0.0038, Accuracy: 7369/10000 (73.69%)\n",
      "Train Epoch: 61 [    0/50000 (  0%)]  Loss: 0.715919\n",
      "Test set: Average loss: 0.0014, Accuracy: 36841/50000 (73.68%)\n",
      "Test set: Average loss: 0.0038, Accuracy: 7327/10000 (73.27%)\n",
      "Train Epoch: 62 [    0/50000 (  0%)]  Loss: 0.808848\n",
      "Test set: Average loss: 0.0015, Accuracy: 36719/50000 (73.44%)\n",
      "Test set: Average loss: 0.0039, Accuracy: 7280/10000 (72.80%)\n",
      "Train Epoch: 63 [    0/50000 (  0%)]  Loss: 0.772494\n",
      "Test set: Average loss: 0.0014, Accuracy: 37040/50000 (74.08%)\n",
      "Test set: Average loss: 0.0039, Accuracy: 7275/10000 (72.75%)\n",
      "Train Epoch: 64 [    0/50000 (  0%)]  Loss: 0.665599\n",
      "Test set: Average loss: 0.0014, Accuracy: 37343/50000 (74.69%)\n",
      "Test set: Average loss: 0.0037, Accuracy: 7381/10000 (73.81%)\n",
      "Train Epoch: 65 [    0/50000 (  0%)]  Loss: 0.786844\n",
      "Test set: Average loss: 0.0014, Accuracy: 36915/50000 (73.83%)\n",
      "Test set: Average loss: 0.0038, Accuracy: 7336/10000 (73.36%)\n",
      "Train Epoch: 66 [    0/50000 (  0%)]  Loss: 0.764827\n",
      "Test set: Average loss: 0.0013, Accuracy: 37752/50000 (75.50%)\n",
      "Test set: Average loss: 0.0036, Accuracy: 7428/10000 (74.28%)\n",
      "Train Epoch: 67 [    0/50000 (  0%)]  Loss: 0.662928\n",
      "Test set: Average loss: 0.0014, Accuracy: 37197/50000 (74.39%)\n",
      "Test set: Average loss: 0.0038, Accuracy: 7369/10000 (73.69%)\n",
      "Train Epoch: 68 [    0/50000 (  0%)]  Loss: 0.740287\n",
      "Test set: Average loss: 0.0014, Accuracy: 37505/50000 (75.01%)\n",
      "Test set: Average loss: 0.0037, Accuracy: 7442/10000 (74.42%)\n",
      "Train Epoch: 69 [    0/50000 (  0%)]  Loss: 0.685809\n",
      "Test set: Average loss: 0.0014, Accuracy: 37585/50000 (75.17%)\n",
      "Test set: Average loss: 0.0037, Accuracy: 7384/10000 (73.84%)\n",
      "Train Epoch: 70 [    0/50000 (  0%)]  Loss: 0.754759\n",
      "Test set: Average loss: 0.0014, Accuracy: 37424/50000 (74.85%)\n",
      "Test set: Average loss: 0.0038, Accuracy: 7343/10000 (73.43%)\n",
      "Train Epoch: 71 [    0/50000 (  0%)]  Loss: 0.722109\n",
      "Test set: Average loss: 0.0014, Accuracy: 37601/50000 (75.20%)\n",
      "Test set: Average loss: 0.0037, Accuracy: 7390/10000 (73.90%)\n",
      "Train Epoch: 72 [    0/50000 (  0%)]  Loss: 0.679178\n",
      "Test set: Average loss: 0.0013, Accuracy: 37929/50000 (75.86%)\n",
      "Test set: Average loss: 0.0037, Accuracy: 7410/10000 (74.10%)\n",
      "Train Epoch: 73 [    0/50000 (  0%)]  Loss: 0.714373\n",
      "Test set: Average loss: 0.0013, Accuracy: 38115/50000 (76.23%)\n",
      "Test set: Average loss: 0.0035, Accuracy: 7507/10000 (75.07%)\n",
      "Train Epoch: 74 [    0/50000 (  0%)]  Loss: 0.759991\n",
      "Test set: Average loss: 0.0013, Accuracy: 38623/50000 (77.25%)\n",
      "Test set: Average loss: 0.0034, Accuracy: 7611/10000 (76.11%)\n",
      "Train Epoch: 75 [    0/50000 (  0%)]  Loss: 0.671592\n",
      "Test set: Average loss: 0.0013, Accuracy: 37983/50000 (75.97%)\n",
      "Test set: Average loss: 0.0037, Accuracy: 7478/10000 (74.78%)\n",
      "Train Epoch: 76 [    0/50000 (  0%)]  Loss: 0.652468\n",
      "Test set: Average loss: 0.0013, Accuracy: 38589/50000 (77.18%)\n",
      "Test set: Average loss: 0.0035, Accuracy: 7588/10000 (75.88%)\n",
      "Train Epoch: 77 [    0/50000 (  0%)]  Loss: 0.599456\n",
      "Test set: Average loss: 0.0013, Accuracy: 38555/50000 (77.11%)\n",
      "Test set: Average loss: 0.0035, Accuracy: 7557/10000 (75.57%)\n",
      "Train Epoch: 78 [    0/50000 (  0%)]  Loss: 0.628667\n",
      "Test set: Average loss: 0.0012, Accuracy: 38730/50000 (77.46%)\n",
      "Test set: Average loss: 0.0035, Accuracy: 7564/10000 (75.64%)\n",
      "Train Epoch: 79 [    0/50000 (  0%)]  Loss: 0.659294\n",
      "Test set: Average loss: 0.0012, Accuracy: 38981/50000 (77.96%)\n",
      "Test set: Average loss: 0.0034, Accuracy: 7647/10000 (76.47%)\n",
      "Train Epoch: 80 [    0/50000 (  0%)]  Loss: 0.610379\n",
      "Test set: Average loss: 0.0012, Accuracy: 39029/50000 (78.06%)\n",
      "Test set: Average loss: 0.0035, Accuracy: 7619/10000 (76.19%)\n",
      "Train Epoch: 81 [    0/50000 (  0%)]  Loss: 0.580177\n",
      "Test set: Average loss: 0.0012, Accuracy: 39363/50000 (78.73%)\n",
      "Test set: Average loss: 0.0033, Accuracy: 7719/10000 (77.19%)\n",
      "Train Epoch: 82 [    0/50000 (  0%)]  Loss: 0.619117\n",
      "Test set: Average loss: 0.0013, Accuracy: 38354/50000 (76.71%)\n",
      "Test set: Average loss: 0.0035, Accuracy: 7583/10000 (75.83%)\n",
      "Train Epoch: 83 [    0/50000 (  0%)]  Loss: 0.572317\n",
      "Test set: Average loss: 0.0012, Accuracy: 39035/50000 (78.07%)\n",
      "Test set: Average loss: 0.0034, Accuracy: 7664/10000 (76.64%)\n",
      "Train Epoch: 84 [    0/50000 (  0%)]  Loss: 0.665018\n",
      "Test set: Average loss: 0.0013, Accuracy: 38131/50000 (76.26%)\n",
      "Test set: Average loss: 0.0036, Accuracy: 7473/10000 (74.73%)\n",
      "Train Epoch: 85 [    0/50000 (  0%)]  Loss: 0.648729\n",
      "Test set: Average loss: 0.0012, Accuracy: 39250/50000 (78.50%)\n",
      "Test set: Average loss: 0.0033, Accuracy: 7679/10000 (76.79%)\n",
      "Train Epoch: 86 [    0/50000 (  0%)]  Loss: 0.550778\n",
      "Test set: Average loss: 0.0012, Accuracy: 39633/50000 (79.27%)\n",
      "Test set: Average loss: 0.0033, Accuracy: 7750/10000 (77.50%)\n",
      "Train Epoch: 87 [    0/50000 (  0%)]  Loss: 0.647763\n",
      "Test set: Average loss: 0.0013, Accuracy: 38626/50000 (77.25%)\n",
      "Test set: Average loss: 0.0035, Accuracy: 7632/10000 (76.32%)\n",
      "Train Epoch: 88 [    0/50000 (  0%)]  Loss: 0.749356\n",
      "Test set: Average loss: 0.0012, Accuracy: 39453/50000 (78.91%)\n",
      "Test set: Average loss: 0.0034, Accuracy: 7696/10000 (76.96%)\n",
      "Train Epoch: 89 [    0/50000 (  0%)]  Loss: 0.598469\n",
      "Test set: Average loss: 0.0012, Accuracy: 39268/50000 (78.54%)\n",
      "Test set: Average loss: 0.0034, Accuracy: 7659/10000 (76.59%)\n",
      "Train Epoch: 90 [    0/50000 (  0%)]  Loss: 0.626627\n",
      "Test set: Average loss: 0.0011, Accuracy: 40007/50000 (80.01%)\n",
      "Test set: Average loss: 0.0032, Accuracy: 7809/10000 (78.09%)\n",
      "Train Epoch: 91 [    0/50000 (  0%)]  Loss: 0.475683\n",
      "Test set: Average loss: 0.0012, Accuracy: 39496/50000 (78.99%)\n",
      "Test set: Average loss: 0.0034, Accuracy: 7670/10000 (76.70%)\n",
      "Train Epoch: 92 [    0/50000 (  0%)]  Loss: 0.641589\n",
      "Test set: Average loss: 0.0011, Accuracy: 39744/50000 (79.49%)\n",
      "Test set: Average loss: 0.0033, Accuracy: 7703/10000 (77.03%)\n",
      "Train Epoch: 93 [    0/50000 (  0%)]  Loss: 0.598238\n",
      "Test set: Average loss: 0.0011, Accuracy: 39782/50000 (79.56%)\n",
      "Test set: Average loss: 0.0033, Accuracy: 7740/10000 (77.40%)\n",
      "Train Epoch: 94 [    0/50000 (  0%)]  Loss: 0.564895\n",
      "Test set: Average loss: 0.0011, Accuracy: 40290/50000 (80.58%)\n",
      "Test set: Average loss: 0.0031, Accuracy: 7843/10000 (78.43%)\n",
      "Train Epoch: 95 [    0/50000 (  0%)]  Loss: 0.527916\n",
      "Test set: Average loss: 0.0011, Accuracy: 40160/50000 (80.32%)\n",
      "Test set: Average loss: 0.0032, Accuracy: 7841/10000 (78.41%)\n",
      "Train Epoch: 96 [    0/50000 (  0%)]  Loss: 0.555315\n",
      "Test set: Average loss: 0.0012, Accuracy: 39311/50000 (78.62%)\n",
      "Test set: Average loss: 0.0033, Accuracy: 7707/10000 (77.07%)\n",
      "Train Epoch: 97 [    0/50000 (  0%)]  Loss: 0.635010\n",
      "Test set: Average loss: 0.0010, Accuracy: 40499/50000 (81.00%)\n",
      "Test set: Average loss: 0.0031, Accuracy: 7807/10000 (78.07%)\n",
      "Train Epoch: 98 [    0/50000 (  0%)]  Loss: 0.543145\n",
      "Test set: Average loss: 0.0010, Accuracy: 40697/50000 (81.39%)\n",
      "Test set: Average loss: 0.0031, Accuracy: 7865/10000 (78.65%)\n",
      "Train Epoch: 99 [    0/50000 (  0%)]  Loss: 0.586984\n",
      "Test set: Average loss: 0.0011, Accuracy: 40277/50000 (80.55%)\n",
      "Test set: Average loss: 0.0032, Accuracy: 7786/10000 (77.86%)\n",
      "Test set: Average loss: 0.0011, Accuracy: 40302/50000 (80.60%)\n",
      "Test set: Average loss: 0.0032, Accuracy: 7786/10000 (77.86%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "77.86"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train(100)\n",
    "test(trainloader)\n",
    "test(testloader)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_by_row(logits, T = 1.0):\n",
    "    mx = np.max(logits, axis=-1, keepdims=True)\n",
    "    exp = np.exp((logits - mx)/T)\n",
    "    denominator = np.sum(exp, axis=-1, keepdims=True)\n",
    "    return exp/denominator\n",
    "\n",
    "def classifier_performance(model, train_loader, test_loader):\n",
    "\n",
    "    output_train_benign = []\n",
    "    train_label = []\n",
    "    for num, data in enumerate(train_loader):\n",
    "        images,labels = data\n",
    "        image_tensor= images.to(device)\n",
    "        img_variable = Variable(image_tensor, requires_grad=True)\n",
    "        output = model.forward(img_variable)\n",
    "\n",
    "        train_label.append(labels.numpy())\n",
    "        output_train_benign.append(softmax_by_row(output.data.cpu().numpy(),T = 1))\n",
    "\n",
    "\n",
    "    train_label = np.concatenate(train_label)\n",
    "    output_train_benign=np.concatenate(output_train_benign)\n",
    "\n",
    "    test_label = []\n",
    "    output_test_benign = []\n",
    "\n",
    "    for num, data in enumerate(test_loader):\n",
    "        images,labels = data\n",
    "\n",
    "        image_tensor= images.to(device)\n",
    "        img_variable = Variable(image_tensor, requires_grad=True)\n",
    "\n",
    "        output = model.forward(img_variable)\n",
    "\n",
    "        test_label.append(labels.numpy())\n",
    "        output_test_benign.append(softmax_by_row(output.data.cpu().numpy(),T = 1))\n",
    "\n",
    "\n",
    "    test_label = np.concatenate(test_label)\n",
    "    output_test_benign=np.concatenate(output_test_benign)\n",
    "\n",
    "\n",
    "    train_acc1 = np.sum(np.argmax(output_train_benign,axis=1) == train_label.flatten())/len(train_label)\n",
    "    test_acc1 = np.sum(np.argmax(output_test_benign,axis=1) == test_label.flatten())/len(test_label)\n",
    "\n",
    "    print('Accuracy: ', (train_acc1, test_acc1))\n",
    "\n",
    "    return output_train_benign, output_test_benign, train_label, test_label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def inference_via_confidence(confidence_mtx1, confidence_mtx2, label_vec1, label_vec2):\n",
    "    \n",
    "    #----------------First step: obtain confidence lists for both training dataset and test dataset--------------\n",
    "    confidence1 = []\n",
    "    confidence2 = []\n",
    "    acc1 = 0\n",
    "    acc2 = 0\n",
    "    for num in range(confidence_mtx1.shape[0]):\n",
    "        confidence1.append(confidence_mtx1[num,label_vec1[num]])\n",
    "        if np.argmax(confidence_mtx1[num,:]) == label_vec1[num]:\n",
    "            acc1 += 1\n",
    "            \n",
    "    for num in range(confidence_mtx2.shape[0]):\n",
    "        confidence2.append(confidence_mtx2[num,label_vec2[num]])\n",
    "        if np.argmax(confidence_mtx2[num,:]) == label_vec2[num]:\n",
    "            acc2 += 1\n",
    "    confidence1 = np.array(confidence1)\n",
    "    confidence2 = np.array(confidence2)\n",
    "    \n",
    "    print('model accuracy for training and test-', (acc1/confidence_mtx1.shape[0], acc2/confidence_mtx2.shape[0]) )\n",
    "    \n",
    "    \n",
    "    #sort_confidence = np.sort(confidence1)\n",
    "    sort_confidence = np.sort(np.concatenate((confidence1, confidence2)))\n",
    "    max_accuracy = 0.5\n",
    "    best_precision = 0.5\n",
    "    best_recall = 0.5\n",
    "    for num in range(len(sort_confidence)):\n",
    "        delta = sort_confidence[num]\n",
    "        ratio1 = np.sum(confidence1>=delta)/confidence_mtx1.shape[0]\n",
    "        ratio2 = np.sum(confidence2>=delta)/confidence_mtx2.shape[0]\n",
    "        accuracy_now = 0.5*(ratio1+1-ratio2)\n",
    "        if accuracy_now > max_accuracy:\n",
    "            max_accuracy = accuracy_now\n",
    "            best_precision = ratio1/(ratio1+ratio2)\n",
    "            best_recall = ratio1\n",
    "    print('membership inference accuracy is:', max_accuracy)\n",
    "    return max_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  (0.8058, 0.7787)\n",
      "model accuracy for training and test- (0.8058, 0.7787)\n",
      "membership inference accuracy is: 0.51606\n",
      "Maximum Accuracy: 0.51606\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import numpy as np\n",
    "import math \n",
    "import scipy\n",
    "import sys  \n",
    "\n",
    "output_train, output_test, train_label, test_label = classifier_performance(model, trainloader, testloader)\n",
    "inference_accuracy=inference_via_confidence(output_train, output_test, train_label, test_label)\n",
    "print(\"Maximum Accuracy:\",inference_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
